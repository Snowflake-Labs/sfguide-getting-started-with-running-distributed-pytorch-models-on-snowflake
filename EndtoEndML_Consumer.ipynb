{
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 32572,
     "sourceId": 3934,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb995b-9816-4408-8241-daf2201edadf",
   "metadata": {
    "collapsed": false,
    "name": "E2E_ML"
   },
   "source": "# End-to-End ML: Deep Learning Recommendation Model\nThis recommendation engine will provide recommendation scores to loyalty customers for each menu item sold by Tasty Bytes food trucks. The output will be used for personalized outreach, increasing the truck brands visited by customers, and increasing traffic to underperforming trucks.\n\nModel training for the recommendation engine leverages distributed training across GPU devices. End-to-end model development and deployment is simplified and streamlined using the following Snowflake features:\n\n- Snowflake Notebooks with GPU Container Runtime (PuPr)\n- Snowflake Feature Store (GA)\n- Snowflake Modeling API (GA) - Preprocessing, Training (PyTorch API PuPr), Evaluation\n- Snowflake Model Registry (GA)\n- Model Deployment from Registry to SPCS (PuPr)"
  },
  {
   "cell_type": "markdown",
   "id": "565d3ba2-7a1a-4f15-b0fe-d5670559b354",
   "metadata": {
    "collapsed": false,
    "name": "SETUP"
   },
   "source": [
    "## Setup\n",
    "Import the Snowflake libraries to support the end-to-end model development and deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell1",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.2.2"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a49f4c-73d5-406a-bd71-aac2b0511a30",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-02T09:50:38.223460Z",
     "iopub.status.busy": "2024-01-02T09:50:38.223195Z",
     "iopub.status.idle": "2024-01-02T09:50:42.516991Z",
     "shell.execute_reply": "2024-01-02T09:50:42.515924Z",
     "shell.execute_reply.started": "2024-01-02T09:50:38.223436Z"
    },
    "language": "python",
    "name": "setup1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "#import sys\n",
    "\n",
    "# Third-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif \n",
    "\n",
    "# Snowflake library imports\n",
    "import streamlit as st\n",
    "from snowflake.ml.modeling.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.pytorch import (\n",
    "    PyTorchTrainer,\n",
    "    ScalingConfig,\n",
    "    WorkerResourceConfig,\n",
    ")\n",
    "#from snowflake.ml.modeling.data import MLRuntimeDataset\n",
    "#from snowflake.ml.data.data_connector import DataConnector\n",
    "from snowflake.ml.modeling.distributors.pytorch import PyTorchDistributor, PyTorchScalingConfig, WorkerResourceConfig\n",
    "from snowflake.ml.data.sharded_data_connector import ShardedDataConnector\n",
    "\n",
    "# from snowflake.ml.modeling.pytorch.context import getContext\n",
    "from snowflake.ml.modeling.distributors.pytorch import get_context\n",
    "\n",
    "from snowflake.ml.feature_store import (\n",
    "FeatureStore,\n",
    "FeatureView,\n",
    ")\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.modeling.metrics import (\n",
    "roc_auc_score,  \n",
    "precision_score, \n",
    "recall_score, \n",
    "confusion_matrix\n",
    ")\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add a query tag to the session. This helps with debugging and performance monitoring.\n",
    "session.query_tag = {\"origin\":\"sf_sit\", \"name\":\"tasty_bytes_e2e_ml\", \"version\":{\"major\":1, \"minor\":0}}\n",
    "\n",
    "db = str(session.get_current_database().strip('\"'))\n",
    "solution_prefix = (db.upper()).split('_PROD')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26c5b8-4940-4ae1-86b2-4e2e3ca23a21",
   "metadata": {
    "collapsed": false,
    "name": "setup2"
   },
   "source": "### GPU Device Info\nView the number of available GPU devices in the notebook.\n\n**Snowflake Feature**: Snowflake GPU Notebooks (PuPr) - easily access GPU compute from a Snowflake Notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe3796-ca94-4e00-95f1-a73436ca05d3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "setup3"
   },
   "outputs": [],
   "source": [
    "# Get device info\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Number of GPU devices available:\", num_gpus)\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        print(\"Device\", i, \":\", torch.cuda.get_device_name(i))\n",
    "    \n",
    "    #Set a default device\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your installation or GPU setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96bdca0-0795-47f4-b062-ff638eb59f31",
   "metadata": {
    "collapsed": false,
    "name": "FEATURE_STORE"
   },
   "source": "## Feature Store\nThe feature store contains feature views for customers, menu items, and purchases. Model features will be accessed from the feature store.\n\n**Snowflake Feature:** Feature Store - Easily find features that work with your data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b30d0-0ea7-4c90-a260-6574c924cfc0",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "feature_store1"
   },
   "outputs": [],
   "source": [
    "# Access feature views\n",
    "FS=FeatureStore(\n",
    "session=session,\n",
    "database=f\"{solution_prefix}_PROD\",\n",
    "    name=\"FS_SCHEMA\",\n",
    "    default_warehouse=f\"{solution_prefix}_DS_WH\")\n",
    "\n",
    "customer_fv : FeatureView = FS.get_feature_view(\n",
    "    name='CUSTOMER_FEATURES',\n",
    "    version='V1'\n",
    ")\n",
    "print(customer_fv)\n",
    "\n",
    "menu_fv : FeatureView = FS.get_feature_view(\n",
    "    name='MENU_FEATURES',\n",
    "    version='V1'\n",
    ")\n",
    "print(menu_fv)\n",
    "\n",
    "purchase_fv : FeatureView = FS.get_feature_view(\n",
    "   name='PURCHASE_FEATURES',\n",
    "   version='V1'\n",
    ")\n",
    "print(purchase_fv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f4158-e371-48d6-937e-056f9b869073",
   "metadata": {
    "collapsed": false,
    "name": "features_store2"
   },
   "source": [
    "### Build Datasets from Feature Store\n",
    "The interaction dataset contains a purchase flag for each menu item/customer pair. This interaction dataset is split into train, validation, and test. The features are brought into the datasets from the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052221d-915e-4c1c-9909-bf63321d8287",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "feature_store3"
   },
   "outputs": [],
   "source": [
    "# Split the interaction dataset and get features from the feature store\n",
    "def create_dataset(spine_df, name):\n",
    "    data = FS.generate_dataset(\n",
    "    name=name,\n",
    "    spine_df=spine_df,\n",
    "    features=[customer_fv, menu_fv, purchase_fv]\n",
    "    )\n",
    "    df = data.read.to_snowpark_dataframe().drop(\"BIRTHDAY_DATE\")\n",
    "    return df\n",
    "    \n",
    "interaction_df = session.table('analytics.loyalty_purchased_items')\n",
    "\n",
    "# Split into train/validation/test\n",
    "datasets = interaction_df.random_split([.1, .1, .8])\n",
    "\n",
    "# Build training tables\n",
    "train_df = create_dataset(datasets[0], \"train\")\n",
    "val_df = create_dataset(datasets[1], \"validation\")\n",
    "    \n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d38fd-ea11-4959-822a-29d93f529410",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "TrainingsetCnt"
   },
   "outputs": [],
   "source": [
    "train_df.count() + val_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcf59d-19fb-4c78-8372-1dcd56702ef2",
   "metadata": {
    "collapsed": false,
    "name": "FEATURE_ENGINEERING"
   },
   "source": [
    "## Feature Engineering\n",
    "The model creates embeddings of categorical (sparse) features. It expects categorical values to be encoded as unique integers. The preprocessing applies label encoding to sparse features and min-max scaling to numeric (dense) features.\n",
    "\n",
    "**Snowflake Feature:** Snowpark ML Modeling API - Feature engineering and preprocessing (GA) - Improve performance and scalability with distributed execution for frequently-used scikit-learn preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e7b770-5544-4902-bb42-dabbef01cb2e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-02T09:50:51.140175Z",
     "iopub.status.busy": "2024-01-02T09:50:51.139850Z",
     "iopub.status.idle": "2024-01-02T09:50:51.146147Z",
     "shell.execute_reply": "2024-01-02T09:50:51.145066Z",
     "shell.execute_reply.started": "2024-01-02T09:50:51.140147Z"
    },
    "language": "python",
    "name": "feature_engineering1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preprocess sparse and dense features\n",
    "sparse_features = ['MENU_ITEM_NAME', \n",
    "                   'MENU_TYPE', \n",
    "                   'TRUCK_BRAND_NAME', \n",
    "                   'ITEM_CATEGORY', \n",
    "                   'ITEM_SUBCATEGORY',\n",
    "                   'CITY',\n",
    "                   'COUNTRY',\n",
    "                   'GENDER',\n",
    "                   'MARITAL_STATUS',]\n",
    "\n",
    "dense_features = ['SALE_PRICE_USD',\n",
    "                  'AGE',\n",
    "                  'AVG_MONTHLY_PURCHASE_AMOUNT',\n",
    "                  'AVG_WEEKLY_PURCHASE_AMOUNT',\n",
    "                  'AVG_YEARLY_PURCHASE_AMOUNT',\n",
    "                 ]\n",
    "\n",
    "label_col = \"PURCHASED\"\n",
    "\n",
    "# Create pipeline\n",
    "pipeline_steps = []\n",
    "\n",
    "# Label encode sparse features\n",
    "for i, feat in enumerate(sparse_features):\n",
    "    le_step = (\n",
    "        f\"LE{i+1}\",\n",
    "        LabelEncoder(input_cols=[feat], output_cols=[feat]),\n",
    "    )\n",
    "    pipeline_steps.append(le_step)\n",
    "\n",
    "# Scale dense features\n",
    "pipeline_steps.append(\n",
    "    (\n",
    "        \"MMS\",\n",
    "        MinMaxScaler(\n",
    "            feature_range=(0, 1),\n",
    "            input_cols=dense_features,\n",
    "            output_cols=dense_features\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=pipeline_steps)\n",
    "train_data = preprocessing_pipeline.fit(train_df).transform(train_df)\n",
    "val_data = preprocessing_pipeline.transform(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049f14d-6c28-4d45-a288-d98383fb520c",
   "metadata": {
    "collapsed": false,
    "name": "feature_engineering2"
   },
   "source": [
    "### Save the Pipeline\n",
    "The saved pipeline will be used for feature transformations in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad088f-a487-4b8b-86ee-ec966c004655",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "feature_engineering3"
   },
   "outputs": [],
   "source": [
    "# Save pipeline to a stage where it can be centrally accessed\n",
    "pipeline_local_path = f'/tmp/dlrm_preprocessor_v1.joblib'\n",
    "joblib.dump(preprocessing_pipeline, open(pipeline_local_path, 'wb'))\n",
    "session.file.put(pipeline_local_path, \n",
    "                 '@ML.ML_STAGE/dlrm_preprocessor_v1.joblib', \n",
    "                 auto_compress=False, \n",
    "                 overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a2d3f-887a-4d8e-a072-cd281373e18a",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "label_encoders1"
   },
   "outputs": [],
   "source": [
    "USE SCHEMA ML;\n",
    "CREATE or replace STAGE UDF_STAGE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc20b90-29fd-4c5b-ae4b-b1e3d6a747d4",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "label_encoders2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = train_df[dense_features + sparse_features + [label_col]]\n",
    "data = data.with_columns(sparse_features,\n",
    "                        [F.col(c).cast(T.StringType()) for c in sparse_features])\n",
    "\n",
    "def serialize_label_encoders(label_encoders):\n",
    "    serialized_label_encoders = {}\n",
    "    for feat, lbe in label_encoders.items():\n",
    "        serialized_label_encoders[feat] = {\n",
    "            'input_cols': lbe.input_cols,\n",
    "            'output_cols': lbe.output_cols,\n",
    "            'classes_': lbe.classes_.tolist()\n",
    "        }\n",
    "    return serialized_label_encoders\n",
    "\n",
    "def save_label_encoders_to_stage(label_encoders, stage_name, dir_name):\n",
    "        serialized_label_encoders = json.dumps(label_encoders)\n",
    "        # Write serialized encoders to a local file first\n",
    "        with open('/tmp/label_encoders.json', 'w') as f:\n",
    "            f.write(serialized_label_encoders)\n",
    "        # Upload the local file to the Snowflake stage\n",
    "        session.file.put('/tmp/label_encoders.json', f'@{stage_name}/{dir_name}',auto_compress=False)\n",
    "        return f'Uploaded to @{stage_name}/{dir_name}'\n",
    "    \n",
    "label_encoders = {}\n",
    "\n",
    "# Iterate over each sparse feature\n",
    "for feat in sparse_features:\n",
    "    # Initialize LabelEncoder for the current feature\n",
    "    lbe = LabelEncoder(input_cols=[feat], output_cols=[feat+'_ENCODED'],drop_input_cols=True)\n",
    "    \n",
    "    # Fit LabelEncoder to the data\n",
    "    lbe.fit(data)\n",
    "    \n",
    "    # Store the LabelEncoder object for reference\n",
    "    label_encoders[feat] = lbe\n",
    "    data = lbe.transform(data)\n",
    "# Serialize label encoders\n",
    "serialized_label_encoders = serialize_label_encoders(label_encoders)\n",
    "stage_name=\"UDF_STAGE\"\n",
    "dir_name=\"dlrm_label_encoders\"\n",
    "# Save serialized label encoders to a file\n",
    "with open('/tmp/label_encoders.json', 'w') as f:\n",
    "    json.dump(serialized_label_encoders, f)\n",
    "save_label_encoders_to_stage(serialized_label_encoders, stage_name, dir_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b75eca0-ad84-4681-83a7-a66e36a00b4f",
   "metadata": {
    "collapsed": false,
    "name": "MODEL"
   },
   "source": [
    "## Model Definition\n",
    "This PyTorch model is a deep learning recommendation model (DLRM). It is being used to provide a recommendation score for every menu item to each loyalty customer. \n",
    "- The embedding layer converts categorical features to dense vectors. \n",
    "- Numeric features are processed through Multi-Layer Perceptron (MLP) layers. \n",
    "- The feature interaction layer captures complex relationships between pairs of input features. \n",
    "- The final dense layers produce the recommendation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeae81-114e-479c-a6b5-c6bd5d7facde",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "model1"
   },
   "outputs": [],
   "source": [
    "# PyTorch DLRM\n",
    "class FeatureInteraction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature_dim = inputs.shape[1]\n",
    "        concat_features = inputs.view(-1, feature_dim, 1)\n",
    "        dot_products = torch.matmul(concat_features, concat_features.transpose(1, 2))\n",
    "        ones = torch.ones_like(dot_products) \n",
    "        mask = torch.triu(ones)\n",
    "        out_dim = feature_dim * (feature_dim + 1) // 2\n",
    "        flat_result = dot_products[mask.bool()]\n",
    "        reshape_result = flat_result.view(-1, out_dim)\n",
    "        return reshape_result\n",
    "\n",
    "class DLRM(nn.Module):\n",
    "    \n",
    "    def __init__(self, sparse_feature_number, dense_feature_number, num_embeddings, embed_dim, bottom_mlp_dims, top_mlp_dims):\n",
    "        super(DLRM, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.EmbeddingBag(num_embeddings=num_embeddings, embedding_dim=embed_dim, mode='sum')\n",
    "        self.layer_feature_interaction = FeatureInteraction()\n",
    "        \n",
    "        self.bottom_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dense_feature_number, bottom_mlp_dims[0]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(bottom_mlp_dims[0], bottom_mlp_dims[1]),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        top_mlp_input_dim = (\n",
    "            (embed_dim + bottom_mlp_dims[1]) \n",
    "            * ((embed_dim + bottom_mlp_dims[1]) + 1) // 2 \n",
    "            + bottom_mlp_dims[1]\n",
    "         )\n",
    "\n",
    "        self.top_mlp = nn.Sequential(\n",
    "            nn.Linear(top_mlp_input_dim, top_mlp_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(top_mlp_dims[0], top_mlp_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(top_mlp_dims[1], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_sparse, x_dense):\n",
    "        # Embedding layer for categorical inputs\n",
    "        embed_x = self.embeddings(x_sparse)\n",
    "        # MLPs for numeric inputs\n",
    "        bottom_mlp_output = self.bottom_mlp(x_dense)\n",
    "        # Combine categical embeddings and MLP outputs\n",
    "        concat_first = torch.cat([bottom_mlp_output, embed_x], dim=-1)\n",
    "        # Get feature interactions\n",
    "        interaction = self.layer_feature_interaction(concat_first)\n",
    "        # Concat interaction outputs with MLP outputs\n",
    "        concat_second = torch.cat([interaction, bottom_mlp_output], dim=-1)\n",
    "        # MLP layers to output \n",
    "        output = self.top_mlp(concat_second)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fac44-6cef-4f6e-9b88-dd4466974e7d",
   "metadata": {
    "collapsed": false,
    "name": "TRAINING"
   },
   "source": "## Model Training\n\nThe model training function deploys the model and data to each device. Gradients are combined and propagated across all devices with each training batch. After each epoch, training and validation losses are averaged across all devices and the model weights are saved. \n\n**Snowflake Feature:** Snowpark ML Modeling API - PyTorch (PuPr) - Perform distributed across GPU devices from a Snowpark DataFrame.\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe955bed-61be-463a-bef7-fd1ed225cdbd",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "training1"
   },
   "outputs": [],
   "source": [
    "# Adjust number of epochs and records in the training data\n",
    "num_epochs = 2\n",
    "training_sample = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbdc98-d252-46f4-a787-9b176b5a5acb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "training2"
   },
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def setup(rank, world_size):\n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "def train_func():\n",
    "    \n",
    "    context = get_context()\n",
    "    rank = context.get_rank()\n",
    "    world_size = context.get_world_size()\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    batch_size = 256\n",
    "\n",
    "    # GET DATA FROM CONTEXT AND SET UP TENSORS\n",
    "    dataset_map = context.get_dataset_map()\n",
    "    training_data = dataset_map[\"train\"].get_shard().to_torch_datapipe(\n",
    "        batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    validation_data = dataset_map[\"val\"].get_shard().to_torch_datapipe(\n",
    "        batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    dataloader = DataLoader(training_data, batch_size=None)\n",
    "    val_dataloader = DataLoader(validation_data, batch_size=None)\n",
    "\n",
    "    # DEFINE MODEL\n",
    "    model = DLRM(\n",
    "        sparse_feature_number=len(sparse_features),\n",
    "        dense_feature_number=len(dense_features),\n",
    "        num_embeddings=142,\n",
    "        embed_dim=128,\n",
    "        bottom_mlp_dims=[256, 128],\n",
    "        top_mlp_dims=[128, 128],\n",
    "    )\n",
    "        \n",
    "    model = model.to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(ddp_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # TRAIN\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        records_processed = 0\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(dataloader):\n",
    "            y = batch_data.pop(label_col).type(torch.float32).to(rank)\n",
    "            \n",
    "            x_sparse = torch.stack(\n",
    "                [tensor.to(torch.int) for key, tensor in batch_data.items() if key in sparse_features],\n",
    "                dim=1\n",
    "            )\n",
    "            x_dense = torch.stack(\n",
    "                [tensor.to(torch.float32) for key, tensor in batch_data.items() if key in dense_features],\n",
    "                dim=1\n",
    "            )\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(x_sparse, x_dense)\n",
    "            loss = criterion(output, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            records_processed += len(y)\n",
    "\n",
    "            if (batch_idx + 1) % 500 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx + 1}: Device {rank} processed {records_processed} records, Epoch Time: {time.time() - start_time:.2f} seconds, Average Training loss: {running_loss / (batch_idx + 1):.4f}\"\n",
    "                )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx + 1}: Device {rank} processed {records_processed} records, Epoch Time: {time.time() - start_time:.2f} seconds, Average Training loss: {running_loss / (batch_idx + 1):.4f}\"\n",
    "        )\n",
    "\n",
    "        # Average loss across devices\n",
    "        running_loss_tensor = torch.tensor(running_loss / (batch_idx + 1), device=rank)\n",
    "        dist.all_reduce(running_loss_tensor)\n",
    "        dist.barrier()\n",
    "        running_loss = running_loss_tensor.item()\n",
    "        running_loss /= world_size\n",
    "\n",
    "        # GET VALIDATION LOSS\n",
    "        ddp_model.eval()\n",
    "        val_loss = 0.0\n",
    "        for val_batch_idx, val_batch_data in enumerate(val_dataloader):\n",
    "            y_val = val_batch_data.pop(label_col).type(torch.float32).to(rank)\n",
    "            x_sparse_val = torch.stack(\n",
    "                [tensor.to(torch.int) for key, tensor in val_batch_data.items() if key in sparse_features],\n",
    "                dim=1\n",
    "            )\n",
    "            x_dense_val = torch.stack(\n",
    "                [tensor.to(torch.float32) for key, tensor in val_batch_data.items() if key in dense_features],\n",
    "                dim=1\n",
    "            )\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                output_val = ddp_model(x_sparse_val, x_dense_val)\n",
    "                loss_val = criterion(output_val, y_val.unsqueeze(1))\n",
    "            \n",
    "            val_loss += loss_val.item()\n",
    "    \n",
    "        # Average validation loss across devices\n",
    "        val_loss_tensor = torch.tensor(val_loss / (val_batch_idx + 1), device=rank)\n",
    "        dist.all_reduce(val_loss_tensor)\n",
    "        dist.barrier()\n",
    "        val_loss = val_loss_tensor.item()\n",
    "        val_loss /= world_size\n",
    "        ddp_model.train()\n",
    "        \n",
    "    \n",
    "        # SAVE MODEL\n",
    "        if rank == 0:\n",
    "            print(f\" Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}, Epoch Time: {time.time() - start_time:.2f} seconds \")\n",
    "            torch.save(model.state_dict(), '/tmp/latest_model.pth')\n",
    "    \n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34267672-efae-46fa-9e53-945221df30fb",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "training3"
   },
   "outputs": [],
   "source": [
    "# Train - Snowflake ML PyTorch API\n",
    "pytroch_trainer = PyTorchDistributor(\n",
    "    train_func=train_func,\n",
    "    scaling_config=PyTorchScalingConfig(\n",
    "        num_nodes=1,\n",
    "        num_workers_per_node=1,\n",
    "        resource_requirements_per_worker=WorkerResourceConfig(num_cpus=0, num_gpus=1),\n",
    "    ),\n",
    ")\n",
    "\n",
    "data_train = ShardedDataConnector.from_dataframe(train_data.limit(training_sample))\n",
    "data_val = ShardedDataConnector.from_dataframe(val_data)\n",
    "\n",
    "out = pytroch_trainer.run(\n",
    "    dataset_map=dict(\n",
    "             train=data_train,\n",
    "             val=data_val\n",
    "         )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4af3f-13f9-42c4-ba09-d44189e85f84",
   "metadata": {
    "collapsed": false,
    "name": "DEPLOYMENT"
   },
   "source": "## Model Deployment\nThe model will be logged to the Snowflake Model Registry. The logged model will then be deployed for inference on Snowpark Container Services (SPCS).\n\n**Snowflake Feature**: Snowflake Model Registry (GA) with SPCS deployment (PuPr) - securely deploy and manage models and their metadata in Snowflake in a flexible compute environment.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a1dc2-021e-42ea-b980-8637b52d0120",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "deployment1"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "def load_model(model_path):\n",
    "    model = DLRM(sparse_feature_number=len(sparse_features),\n",
    "                 dense_feature_number=len(dense_features),\n",
    "                 num_embeddings=142,\n",
    "                 embed_dim=128,\n",
    "                 bottom_mlp_dims=[256, 128],\n",
    "                 top_mlp_dims=[128, 128])\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load saved model\n",
    "model = load_model('/tmp/latest_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a78965-eabe-43c1-b761-6e7aa2858b56",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "deployment2"
   },
   "outputs": [],
   "source": [
    "# Register the model to the Snowflake model registry.\n",
    "registry = Registry(session=session, database_name=f\"{solution_prefix}_PROD\", schema_name=\"REGISTRY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9044e8-4f2c-492f-99b2-737ebd84ad79",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "deployment3"
   },
   "outputs": [],
   "source": [
    "sample_input = train_data.limit(1).to_pandas()\n",
    "x_sparse = torch.tensor(sample_input[sparse_features].values, dtype=torch.int)\n",
    "x_dense = torch.tensor(sample_input[dense_features].values, dtype=torch.float32)\n",
    "\n",
    "# Log model to registry\n",
    "model_ref = registry.log_model(\n",
    "    model,\n",
    "    model_name=\"RecModelDemo\",\n",
    "    version_name=\"V1\",\n",
    "    sample_input_data=[x_sparse[0].unsqueeze(0), x_dense[0].unsqueeze(0)],\n",
    "    options={'relax_version': True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e98a0e-c524-4d11-99e8-dd63130c30a3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "deployment4",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": "model_ref.create_service(service_name=\"TB_REC_SERVICE_DEMO_PREDICT\",\n                  service_compute_pool=\"TASTYBYTESENDTOENDML_DEPLOY_POOL\",\n                  image_repo=\"TASTYBYTESENDTOENDML_PROD.REGISTRY.IMAGE_REPO\",\n                  build_external_access_integration=\"TASTYBYTESENDTOENDML_CONDA_ACCESS_INTEGRATION\")"
  },
  {
   "cell_type": "markdown",
   "id": "791dea7c-2cd3-4023-b5d1-4e22ab69196e",
   "metadata": {
    "collapsed": false,
    "name": "INFERENCE"
   },
   "source": [
    "## Model Inference & Evaluation\n",
    "Inference of the test data will be completed using the model deployed to SPCS running on a dedicated compute pool. Features for the test data will be accessed from the feature store and the preprocessing pipeline will transform the data as required.\n",
    "\n",
    "**Model Output**: \n",
    "The model will output a score based on the input features. The higher the score, the more highly recommended a menu item is for that customer. A binary prediction is created from the score to evaluate the model's performance (1 if the score above 0.5, 0 otherwise).\n",
    "\n",
    "**Snowflake Features:** \n",
    "- Inference on SPCS (PrPr) - Run inference against the model deployed to a container environment with a dedicated compute pool.\n",
    "- Snowpark ML Modeling API - Evaluation Metrics (GA) - Improve performance and scalability with distributed execution for frequently-used scikit-learn preprocessing functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ac39e-1202-4ecd-be7c-7be4bb7dddbc",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "inference1"
   },
   "outputs": [],
   "source": [
    "ALTER FUNCTION IF EXISTS TB_REC_SERVICE_DEMO_PREDICT(ARRAY, ARRAY) SET MAX_BATCH_ROWS = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e125048-5dc6-4a32-92ce-012f1f4a83ef",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "inference2"
   },
   "outputs": [],
   "source": [
    "# Get features from feature store\n",
    "test_df = FS.retrieve_feature_values(\n",
    "    spine_df=datasets[2],\n",
    "features=[customer_fv, menu_fv, purchase_fv]\n",
    ")\n",
    "#test_df_subset = test_df.sample(100000)\n",
    "test_df_subset = test_df.sample(n=10000)\n",
    "\n",
    "# Preprocess\n",
    "test_data = preprocessing_pipeline.transform(test_df_subset)\n",
    "\n",
    "# Predict\n",
    "eval_df = test_data.select(\n",
    "    \"customer_id\",\n",
    "    \"menu_item_name\",\n",
    "    \"purchased\",\n",
    "    F.call_udf(\n",
    "        \"TB_REC_SERVICE_DEMO_PREDICT_FORWARD\", \n",
    "        F.array_construct(*sparse_features),\n",
    "        F.array_construct(*dense_features),\n",
    "    )[\"output_feature_0\"][0].alias(\"prediction\"),\n",
    "    F.iff(F.col(\"prediction\") >=0.5, 1, 0).alias(\"binary_prediction\")\n",
    ").order_by(\"prediction\", ascending=0).cache_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7bf322-f7bf-4078-ac69-d41303596b90",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "inference3"
   },
   "outputs": [],
   "source": [
    "eval_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a30bf-2dce-4caf-838f-a3dc9381fa8f",
   "metadata": {
    "collapsed": false,
    "name": "EVALUATION"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "A strong recommendation model should recommend most of the purchased items (this was the training indicator that an item was of interest to the customer). On the other hand, unpurchased items are not always indications of disinterest. The goal of the recommendation engine is to identify unpurchased items that could be of interest to the customer, which would require \"misclassifying\" unpurchased items. Ideally, we want a high recall and a portion of unpurchased items to be recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49dbd2-9728-443f-a562-8b65fc8318e7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "evaluation1"
   },
   "outputs": [],
   "source": [
    "# Get Evaluation Metrics\n",
    "cols = st.columns(3)\n",
    "cols[0].metric(\"AUC\", round(roc_auc_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_score_col_names=\"PREDICTION\"),3))\n",
    "cols[1].metric(\"Recall\", round(recall_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\"),3))\n",
    "cols[2].metric(\"Precision\", round(precision_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\"),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91698a5-2cb7-4183-8eb8-8f5b8295ede2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "evaluation2"
   },
   "outputs": [],
   "source": [
    "m = registry.get_model(\"RecModelDemo\")\n",
    "mv=m.version(\"v1\")\n",
    "mv.set_metric(\"AUC\", round(roc_auc_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_score_col_names=\"PREDICTION\"),3))\n",
    "mv.set_metric(\"Recall\", round(recall_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\"),3))\n",
    "mv.set_metric(\"Precision\", round(precision_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\"),3))\n",
    "#m.set_tag(\"live_version\", \"v1\")\n",
    "m.description = \"Provides menu recommendations for Tasty bytes business\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9b9d4-0576-472c-a4fb-4ea7d57e3a71",
   "metadata": {
    "collapsed": false,
    "name": "SUMMARY"
   },
   "source": [
    "## Summary\n",
    "This notebook leveraged a GPU compute pool, unlocking the ability to work with data at scale and deep learning models. End-to-end, this workflow used a Snowpark DataFrame and leveraged the following features to simplify development and deployment:\n",
    "- Snowflake Notebooks with GPU Container Runtime (PrPr)\n",
    "- Snowflake Feature Store (PuPr)\n",
    "- Snowflake Modeling API (GA) - Preprocessing, Training (PyTorch API PrPr), Evaluation\n",
    "- Snowflake Model Registry (GA)\n",
    "- Model Deployment to SPCS (PrPr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "close_session"
   },
   "outputs": [],
   "source": "session.close()",
   "id": "ce110000-1111-2222-3333-ffffff000001"
  }
 ]
}